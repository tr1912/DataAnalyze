
	将原始数据转换为更加纯净的数据集，数据集越纯净，则越便于让模型总结出数据集中潜在的规律，从而提高对未知数据预测的准确性。

意义：
* 直接影响模型实现预测或者分类的效果
* 流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

# 1、特征抽取

* 将非数值型特征转换成数值型特征的过程，称为“特征值化”。
* 对于非数值型特征，可以分为如下两种类别：
	* 无序分类变量：说明事物类别的一个名称，如性别(男女)，二者无大小、顺序之分，还有如血型、民族等
	* 有序分类变量：说明事物类型的一个名称，但是有次序之分，例如满意度(满意，一般，不满意)，三者有序且有大小之分

实现方式：
* map映射（针对有序的）
	* ![[Pasted image 20241112012337.png]]
* one-hot编码（针对无序的）
	* ![[Pasted image 20241112012359.png]]
	* 

特征值化的实现方式选择：
* 方案1：
	* 有序分类变量使用map映射，无序分类变量使用one-hot映射
* 方案2：
	* 如果分数值型特征组成元素为2个，使用map映射，否则使用one-hot编码
* 终极方案：
	* 在对数据不是很理解的情况下，依次使用map映射和one-hot进行特征值化，根据模型的不同表现，选择合适的特征值化的方法


# 2、特征预处理

## 无量纲化

在机器学习算法实践中，我们往往有着将不同分布的数据转换到某个特定分布的需求，这种需求统称为将数据“无量纲化”

* 归一化
	* ![[Pasted image 20241113010128.png]]
* 标准化
	* ![[Pasted image 20241113010418.png]]

归一化进行无量纲化：
``` python
from sklearn.preprocessing import MinMaxScaler
# 1.创建一个工具对象
tool = MinMaxScalar()
# 2.使用创建好的对象进行归一化操作，并返回结果
m_data = tool.fit_transform(data)
m_data

```

使用标准化进行无量纲化：
``` python
from sklearn.preprocessing import StandardScaler
#1.创建工具对象
s_tool = StandardScaler()
#2.使用工具对象进行标准化操作，并返回结果
ret = s_tool.fit_transform(data)

print(ret)
```

归一化容易实现，效率高，标准化兼容性比较高
建议都尝试，看看那种对于模型影响较为正向
# 3、特征选择

  * 概述：从特征中选择出有意义、对模型有帮助的特征作为最终的机器学习输入数据。
  * 原因：去除原始数据中的冗余噪点和特征，从而保留下重要特征。
	  * 冗余：部分特征的相关度较高，容易消耗计算机的性能(保留一个就行)
	  * 噪点：部分特征对预测结果有偏执影响(去除)。

## 实现

对不相关特征进行主观舍弃
	如何？
	基于相关工具进行特征选择：
	* Filter方差过滤
	* PCA降维
	* 相关系数

### 1.方差过滤

原理：通过特征本身的方差来筛选特征
比如一个特征的方差很小，就表示样本在这个特征上基本没有差异，那这个特征对于样本区分来说没有什么作用。

#### 工具：方差过滤接口

``` python
from sklearn.feature_selection import VarianceThreadhold

```

实战：
```python
# 提取样本特征
import numpy as np
from sklearn import datasets
cancer = datasets.breast_Cancer()

# 查看特征维度
feature = cancer.data
target = cancer.target

feature.shape

# 查看特征方差，基于方差进行排序后进行查看
np.sort(feature.var(axis=0))
from sklearn.feature_selection import VarianceThreshold

min_var = 1.5e-3   # 指定方差阈值
#创建工具对象，参数Threshold表示过滤掉低于该值对应的特征列
tool = VarianceThreshold(threshold = min_var)
# 根据指定的阈值threshold过滤低于该值对应的特征列
m_feature = tool.fit_transform(feature)
m_feature.shape
# 如果指定阈值比较麻烦，可以直接使用特征方差的中位数作为阈值

# 查看特征方差的中位数
value = np.median(feature.var(axis=o))
#创建工具对象，过滤低于这个值的特征列
tool = VarianceThreshold(threshold=valu)
# 根据指定阈值threshold过滤低于阈值的特征
ret = tool.fit_transform(feture)
ret.shape
```


### 2.PCA降维

  通俗来讲：使用低维度事物在信息损失最小的情况下表示高维度事物。
* 目的：特征数量上百千的时候，考虑数据的优化，使数据维度压缩，尽可能降低源数据的维度（复杂度），损失少量信息。
* 作用：可以削减回归分析或者聚类分析中特征的数量

#### 原理
![[Pasted image 20241117172026.png]]
![[Pasted image 20241117172044.png]]
![[Pasted image 20241117172138.png]]

降维之前和降维之后，方差值是一样的，就说明是可以降维的。

代码实战：

``` python
# 提取样本数据
import numpy as np
from sklearn import datasets
cancer = datasets.load_wine()

feature = cancer.data
target = cancer.target

feature.shape
# 导入PCA工具对象
from sklearn.decomposition import PCA
# n_components 表示最后要保留几个维度特征
tool = PCA(n_components=9)
ret = tool.fit_transform(feature)
ret.shape
```

### 3.相关系数

* 当我们想要研究两组或两组以上的数据之间有什么关系的时候，就会用到相关分析法。
* 相关性分为两类
	* 正相关：一个变量的增长会引起另一个变量的增长
	* 负相关：一个变量的增长反而会引起另一个变量的下降
* 相关系数r的取值范围[-1, 1]
	* r=1 代表完全正相关
	* r=-1 代表完全负相关
	* |r|>0.6 代表相关性很强

代码实现：

```python
import numpy as np
from sklearn import datasets
cancer = datasets.load_iris()

feature = cancer.data
target = cancer.target

feature.shape

# 计算feature之间的相关系数

feature = pd.DataFrame(data=feature)
ret = feature.corr().abs()
ret

# 基于热力图
import seaborn as sns
sns.heatmap(ret, cmap='Reds', annot=True)

```


