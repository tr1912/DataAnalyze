# 1.数据集获取

## 数据集获取接口

* sklearn.datasets.load_\*()   获取小规模数据集
* sklearn.datasets.fetch_\*()  获取大规模的数据集

``` python
from sklearn import datasets
# 获取房价数据集
house = datasets.load_boston()
house
# 提取样本特征
feature = house.data
target = house.target

feature.shape

target.shape

# 获取红酒数据集
wine = datasets.load_wine()
wine

feature = wine.data
target = wine.target

# 大规模数据集
house = datasets.fetch_california_housing()
house

# 提取样本特征
feature = house.data
target = house.target

# 提取人脸数据
faces = datasets.fetch_oliveti_faces()
faces

feature = faces.data
target = faces.target

```

# 数据集切分

q：获取到数据集之后，是否需要把全部数据都用来训练模型？
a：不是的


我们需要将原先的样本数据拆分成两个部分：
 * 训练集：训练模型
 * 测试集：评估模型

## 数据集切分接口

from sklearn.model_selection import train_test_split
train_test_split(x, y, test_size, random_state)

参数：
* x：特征
* y：目标
* test_size：测试集的比例
* random_state：打乱的随机种子
* 返回值：训练特征、测试特征、训练目标、测试目标

``` python
from sklearn.model_selection import train_test_split
import pandas as pd

films = pd.read_excel('./datasets/my_file.xlsx')
films

feature = films[['Action Lens', 'Love Lens']]
target = films['target']

# 数据集切分
	#x_train,y_train: 训练样本
	#x_test, y_test: 测试样本
x_train,x_test,y_train,y_test =  train_test_split(feature, target, test_size=0.2, random_state = 110)
# 查看训练集数据和测试集数据的样本数量和样本量占比
x_train.shape
x_test.shape


```